import tensorflow as tf
import os
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import time

# ===== DATASET YÃœKLEME VE HAZIRLAMA =====
print("ğŸ” Oxford-IIIT Pet Dataset Analizi BaÅŸlÄ±yor...")
print("=" * 50)

# Dataset yollarÄ±
dataset_path = "dataset/images"
IMG_SIZE = 224
BATCH_SIZE = 32
AUTOTUNE = tf.data.AUTOTUNE

# SÄ±nÄ±f isimleri (Oxford-IIIT Pet'te 37 sÄ±nÄ±f)
class_names = [
    'Abyssinian', 'American_bulldog', 'American_pit_bull_terrier', 'Basset_hound',
    'Beagle', 'Bengal', 'Birman', 'Bombay', 'Boxer', 'British_shorthair', 'Chihuahua',
    'Egyptian_mau', 'English_cocker_spaniel', 'English_setter', 'German_shorthaired',
    'Great_pyrenees', 'Havanese', 'Japanese_chin', 'Keeshond', 'Leonberger',
    'Maine_coon', 'Miniature_pinscher', 'Newfoundland', 'Persian', 'Pomeranian',
    'Pug', 'Ragdoll', 'Russian_blue', 'Saint_bernard', 'Samoyed', 'Scottish_terrier',
    'Shiba_inu', 'Siamese', 'Sphynx', 'Staffordshire_bull_terrier', 'Wheaten_terrier',
    'Yorkshire_terrier'
]

print(f"ğŸ“Š Toplam SÄ±nÄ±f SayÄ±sÄ±: {len(class_names)}")
print(f"ğŸ± Kedi TÃ¼rleri: {len([c for c in class_names if c in ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_shorthair', 'Egyptian_mau', 'Maine_coon', 'Persian', 'Ragdoll', 'Russian_blue', 'Siamese', 'Sphynx']])}")
print(f"ğŸ• KÃ¶pek TÃ¼rleri: {len([c for c in class_names if c not in ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_shorthair', 'Egyptian_mau', 'Maine_coon', 'Persian', 'Ragdoll', 'Russian_blue', 'Siamese', 'Sphynx']])}")

def load_and_preprocess_dataset():
    """Dataset'i yÃ¼kle ve Ã¶n iÅŸle"""
    print("\nğŸ“ Dataset yÃ¼kleniyor...")
    
    images = []
    labels = []
    
    for class_idx, class_name in enumerate(class_names):
        print(f"  - {class_name} sÄ±nÄ±fÄ± iÅŸleniyor...")
        
        # Dosya adlarÄ±ndan sÄ±nÄ±f isimlerini Ã§Ä±kar
        for filename in os.listdir(dataset_path):
            if filename.startswith(class_name.lower()) and filename.endswith('.jpg'):
                try:
                    img_path = os.path.join(dataset_path, filename)
                    image = Image.open(img_path).convert('RGB')
                    image = image.resize((IMG_SIZE, IMG_SIZE))
                    image_array = np.array(image)
                    
                    images.append(image_array)
                    labels.append(class_idx)
                except Exception as e:
                    print(f"    Hata: {filename} - {e}")
    
    images = np.array(images)
    labels = np.array(labels)
    
    print(f"âœ… Dataset yÃ¼klendi: {len(images)} resim, {len(np.unique(labels))} sÄ±nÄ±f")
    return images, labels

def create_tf_dataset(images, labels, train_split=0.8):
    """TensorFlow dataset oluÅŸtur (shuffle ve stratified split ile)"""
    print("\nğŸ”„ TensorFlow Dataset oluÅŸturuluyor...")
    
    # Stratified split
    from sklearn.model_selection import StratifiedShuffleSplit
    sss = StratifiedShuffleSplit(n_splits=1, test_size=1-train_split, random_state=42)
    for train_index, test_index in sss.split(images, labels):
        train_images, test_images = images[train_index], images[test_index]
        train_labels, test_labels = labels[train_index], labels[test_index]
    
    def preprocess_image(img, label):
        img = tf.cast(img, tf.float32) / 255.0  # Normalizasyon
        return img, label
    
    train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
    train_ds = train_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)
    train_ds = train_ds.shuffle(1000)
    train_ds = train_ds.batch(BATCH_SIZE)
    train_ds = train_ds.prefetch(AUTOTUNE)
    
    test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))
    test_ds = test_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)
    test_ds = test_ds.batch(BATCH_SIZE)
    test_ds = test_ds.prefetch(AUTOTUNE)
    
    print(f"âœ… Train: {len(train_images)}, Test: {len(test_images)}")
    return train_ds, test_ds

# ===== FARKLI TRANSFER LEARNING MODELLERÄ° =====

def create_model(base_model_name, num_classes=37):
    """FarklÄ± base model'lerle model oluÅŸtur"""
    
    if base_model_name == "MobileNetV2":
        base_model = tf.keras.applications.MobileNetV2(
            input_shape=(IMG_SIZE, IMG_SIZE, 3),
            include_top=False,
            weights='imagenet'
        )
        model_type = "Hafif ve HÄ±zlÄ±"
        
    elif base_model_name == "ResNet50":
        base_model = tf.keras.applications.ResNet50(
            input_shape=(IMG_SIZE, IMG_SIZE, 3),
            include_top=False,
            weights='imagenet'
        )
        model_type = "Derin ve GÃ¼Ã§lÃ¼"
        
    elif base_model_name == "EfficientNetB0":
        base_model = tf.keras.applications.EfficientNetB0(
            input_shape=(IMG_SIZE, IMG_SIZE, 3),
            include_top=False,
            weights='imagenet'
        )
        model_type = "Modern ve Verimli"
        
    elif base_model_name == "VGG16":
        base_model = tf.keras.applications.VGG16(
            input_shape=(IMG_SIZE, IMG_SIZE, 3),
            include_top=False,
            weights='imagenet'
        )
        model_type = "Klasik ve GÃ¼venilir"
    
    # Base model'i dondur (freeze)
    base_model.trainable = False
    
    # Kendi modelimizi ekle
    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    
    return model, model_type

def train_and_evaluate_model(model, model_name, train_ds, test_ds, epochs=30):
    """Modeli eÄŸit ve deÄŸerlendir (daha fazla epoch, dÃ¼ÅŸÃ¼k learning rate)"""
    print(f"\nğŸš€ {model_name} eÄŸitimi baÅŸlÄ±yor...")
    
    # Model derleme
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # EÄŸitim sÃ¼resini Ã¶lÃ§
    start_time = time.time()
    
    # Model eÄŸitimi
    history = model.fit(
        train_ds,
        epochs=epochs,
        validation_data=test_ds,
        verbose=1
    )
    
    training_time = time.time() - start_time
    
    # Test performansÄ±
    test_loss, test_accuracy = model.evaluate(test_ds, verbose=0)
    
    # Model boyutu
    model_size = model.count_params()
    
    print(f"âœ… {model_name} eÄŸitimi tamamlandÄ±!")
    print(f"   - Test DoÄŸruluÄŸu: {test_accuracy*100:.2f}%")
    print(f"   - EÄŸitim SÃ¼resi: {training_time:.2f} saniye")
    print(f"   - Model Boyutu: {model_size:,} parametre")
    
    return {
        'model': model,
        'history': history,
        'test_accuracy': test_accuracy,
        'training_time': training_time,
        'model_size': model_size
    }

# ===== ANA ANALÄ°Z =====

def main():
    """Ana analiz fonksiyonu"""
    
    # Dataset yÃ¼kle
    images, labels = load_and_preprocess_dataset()
    train_ds, test_ds = create_tf_dataset(images, labels)
    
    # Test edilecek modeller
    models_to_test = ["MobileNetV2", "ResNet50", "EfficientNetB0", "VGG16"]
    results = {}
    
    print("\n" + "="*60)
    print("ğŸ¯ TRANSFER LEARNING MODEL KARÅILAÅTIRMASI")
    print("="*60)
    
    for model_name in models_to_test:
        print(f"\nğŸ“Š {model_name} analizi...")
        
        # Model oluÅŸtur
        model, model_type = create_model(model_name)
        
        print(f"   Model Tipi: {model_type}")
        print(f"   Base Model: {model_name}")
        print(f"   Parametre SayÄ±sÄ±: {model.count_params():,}")
        
        # Modeli eÄŸit ve deÄŸerlendir
        result = train_and_evaluate_model(model, model_name, train_ds, test_ds)
        results[model_name] = result
        
        # Modeli kaydet
        model.save(f"oxford_pets_{model_name.lower()}.h5")
        print(f"   ğŸ’¾ Model kaydedildi: oxford_pets_{model_name.lower()}.h5")
    
    # ===== SONUÃ‡LARI KARÅILAÅTIR =====
    print("\n" + "="*60)
    print("ğŸ“ˆ KARÅILAÅTIRMA SONUÃ‡LARI")
    print("="*60)
    
    comparison_data = []
    for model_name, result in results.items():
        comparison_data.append({
            'Model': model_name,
            'DoÄŸruluk (%)': result['test_accuracy'] * 100,
            'EÄŸitim SÃ¼resi (s)': result['training_time'],
            'Parametre SayÄ±sÄ±': result['model_size']
        })
    
    # SonuÃ§larÄ± tablo halinde gÃ¶ster
    for data in comparison_data:
        print(f"ğŸ”¸ {data['Model']:15} | "
              f"DoÄŸruluk: {data['DoÄŸruluk (%)']:6.2f}% | "
              f"SÃ¼re: {data['EÄŸitim SÃ¼resi (s)']:6.2f}s | "
              f"Parametre: {data['Parametre SayÄ±sÄ±']:8,}")
    
    # En iyi modeli belirle
    best_model = max(results.items(), key=lambda x: x[1]['test_accuracy'])
    print(f"\nğŸ† EN Ä°YÄ° MODEL: {best_model[0]}")
    print(f"   DoÄŸruluk: {best_model[1]['test_accuracy']*100:.2f}%")
    
    # Production iÃ§in Ã¶neriler
    print("\n" + "="*60)
    print("ğŸ’¡ PRODUCTION Ã–NERÄ°LERÄ°")
    print("="*60)
    print("ğŸ”¸ Mobil Uygulama: MobileNetV2 (hÄ±zlÄ±, kÃ¼Ã§Ã¼k)")
    print("ğŸ”¸ Web UygulamasÄ±: EfficientNetB0 (denge)")
    print("ğŸ”¸ YÃ¼ksek DoÄŸruluk: ResNet50 (gÃ¼Ã§lÃ¼)")
    print("ğŸ”¸ Klasik YaklaÅŸÄ±m: VGG16 (gÃ¼venilir)")
    
    return results

if __name__ == "__main__":
    results = main() 